# ─── Server ───────────────────────────────────────────────────────────────────
WORKER_PORT=3456
WORKER_SECRET=change-this-to-a-secure-random-string
NODE_ENV=development

# ─── AI Provider ──────────────────────────────────────────────────────────────
# Choose how the worker connects to AI. Options:
#   claude-cli        Use Claude Code CLI (requires: claude auth)
#   anthropic-api     Use Anthropic API directly (requires: API_KEY)
#   openai-compatible Use any OpenAI-compatible server (requires: API_BASE_URL + API_KEY)
#   ollama            Local Ollama server (requires: MODEL, no key needed)

PROVIDER=claude-cli

# Model to use (ignored for claude-cli, which uses whatever is configured in claude auth)
# Examples: claude-opus-4-6, claude-sonnet-4-5-20250929, gpt-4o, llama3.2
MODEL=claude-opus-4-6

# API key — required for anthropic-api and openai-compatible providers
# For anthropic-api: get from https://console.anthropic.com
# For openai-compatible: get from your provider (OpenAI, OpenRouter, etc.)
# For ollama or claude-cli: leave blank
API_KEY=

# Base URL — required for openai-compatible; auto-set for ollama
# Examples:
#   Ollama local:  http://localhost:11434/v1   (set automatically when PROVIDER=ollama)
#   LM Studio:     http://localhost:1234/v1
#   OpenRouter:    https://openrouter.ai/api/v1
#   OpenAI:        https://api.openai.com/v1
API_BASE_URL=
